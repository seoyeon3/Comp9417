{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18572,
     "status": "ok",
     "timestamp": 1650105714434,
     "user": {
      "displayName": "whatififif whatififif",
      "userId": "16442723417007489448"
     },
     "user_tz": -600
    },
    "id": "UjfAlxABB79j",
    "outputId": "3f966c57-cc7a-4c04-b242-18640d0186fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "##################################################################\n",
    "# This code gets the predictions on real test sample by one ViT model\n",
    "# and save the result as numpy file.\n",
    "##################################################################\n",
    "#################################\n",
    "# access to google drive\n",
    "#################################\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5113,
     "status": "ok",
     "timestamp": 1650105719538,
     "user": {
      "displayName": "whatififif whatififif",
      "userId": "16442723417007489448"
     },
     "user_tz": -600
    },
    "id": "0KVcaZ_sITna",
    "outputId": "7bb32faf-d38b-4f01-91e6-41ef67246e1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch_pretrained_vit\n",
      "  Downloading pytorch-pretrained-vit-0.0.7.tar.gz (13 kB)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_vit) (1.10.0+cu111)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->pytorch_pretrained_vit) (4.1.1)\n",
      "Building wheels for collected packages: pytorch-pretrained-vit\n",
      "  Building wheel for pytorch-pretrained-vit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pytorch-pretrained-vit: filename=pytorch_pretrained_vit-0.0.7-py3-none-any.whl size=11132 sha256=6ba801609b00c2943e98f6336906b064a0cac01ba10f924791e3bf00a9c0c2c6\n",
      "  Stored in directory: /root/.cache/pip/wheels/87/1d/d1/c6852ef6d18565e5aee866432ab40c6ffbd3411d592035cddb\n",
      "Successfully built pytorch-pretrained-vit\n",
      "Installing collected packages: pytorch-pretrained-vit\n",
      "Successfully installed pytorch-pretrained-vit-0.0.7\n"
     ]
    }
   ],
   "source": [
    "#################################\n",
    "# import ViT pretrained model\n",
    "#################################\n",
    "!pip install pytorch_pretrained_vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8006,
     "status": "ok",
     "timestamp": 1650105727540,
     "user": {
      "displayName": "whatififif whatififif",
      "userId": "16442723417007489448"
     },
     "user_tz": -600
    },
    "id": "1qUnueFt_ebC",
    "outputId": "7b3c0ad5-1dd6-498f-f1e8-d32282d9d8c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.10.0+cu111\n",
      "Torchvision Version:  0.11.1+cu111\n"
     ]
    }
   ],
   "source": [
    "#################################\n",
    "# import modules\n",
    "#################################\n",
    "from __future__ import print_function \n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from pytorch_pretrained_vit import ViT\n",
    "from PIL import Image\n",
    "import glob\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1650105727541,
     "user": {
      "displayName": "whatififif whatififif",
      "userId": "16442723417007489448"
     },
     "user_tz": -600
    },
    "id": "qAcQ03Nj_qpx"
   },
   "outputs": [],
   "source": [
    "#################################\n",
    "# sample paths\n",
    "# Top level data directory. \n",
    "# Here we assume the format of the directory conforms to the ImageFolder structure\n",
    "#################################\n",
    "\n",
    "# Top level data directory. Here we assume the format of the directory conforms \n",
    "#   to the ImageFolder structure\n",
    "# data_dir = \"/content/drive/MyDrive/shared/samples\"\n",
    "# data_dir = \"/content/drive/MyDrive/shared/samples_small\"\n",
    "# data_dir = \"/content/drive/MyDrive/shared/samples_x299\"  # for indeption\n",
    "# data_dir = \"/content/drive/MyDrive/shared/samples_x384\"  # for vit\n",
    "# data_dir = \"/content/drive/MyDrive/shared/samples_x384_all\"  # for vit\n",
    "# data_dir = \"/content/drive/MyDrive/shared/samples_x384_all_5\"  # for vit\n",
    "# data_dir = \"/content/drive/MyDrive/shared/samples_x384_all_4\"  # for vit\n",
    "# data_dir = \"/content/drive/MyDrive/shared/samples_x384_all_3\"  # for vit\n",
    "# data_dir = \"/content/drive/MyDrive/shared/samples_x384_all_2\"  # for vit\n",
    "data_dir = \"/content/drive/MyDrive/shared/samples_x384_all_1\"  # for vit\n",
    "# data_dir = \"/content/drive/MyDrive/shared/samples_x384_small_5\"  # for vit\n",
    "# data_dir = \"/content/drive/MyDrive/samples2\"  # for vgg\n",
    "# data_dir = \"/content/drive/MyDrive/shared/samples_x384_small_4\"  # for vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1650105727542,
     "user": {
      "displayName": "whatififif whatififif",
      "userId": "16442723417007489448"
     },
     "user_tz": -600
    },
    "id": "RdR7nfkm_sqT"
   },
   "outputs": [],
   "source": [
    "#################################\n",
    "# various model to test\n",
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "#################################\n",
    "\n",
    "# model_name = \"resnet\"\n",
    "# model_name = \"alexnet\"\n",
    "# model_name = \"vgg\"\n",
    "# model_name = \"squeezenet\"\n",
    "# model_name = \"densenet\"\n",
    "# model_name = \"inception\"\n",
    "model_name = \"vit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1650105727544,
     "user": {
      "displayName": "whatififif whatififif",
      "userId": "16442723417007489448"
     },
     "user_tz": -600
    },
    "id": "64o83vBZ_uuD"
   },
   "outputs": [],
   "source": [
    "#################################\n",
    "# Number of classes in the dataset\n",
    "#################################\n",
    "\n",
    "num_classes = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1650105727545,
     "user": {
      "displayName": "whatififif whatififif",
      "userId": "16442723417007489448"
     },
     "user_tz": -600
    },
    "id": "7Z_XfY9p_wv1"
   },
   "outputs": [],
   "source": [
    "#################################\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "#################################\n",
    "# batch_size = 43\n",
    "batch_size = 5  # for ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1650105727545,
     "user": {
      "displayName": "whatififif whatififif",
      "userId": "16442723417007489448"
     },
     "user_tz": -600
    },
    "id": "6sSU3i9ZAJnS"
   },
   "outputs": [],
   "source": [
    "#################################\n",
    "# Number of epochs to train for\n",
    "#################################\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1650105727546,
     "user": {
      "displayName": "whatififif whatififif",
      "userId": "16442723417007489448"
     },
     "user_tz": -600
    },
    "id": "tV5bHBoSANKh"
   },
   "outputs": [],
   "source": [
    "#################################\n",
    "# Flag for feature extracting. When False, we finetune the whole model,\n",
    "#   when True we only update the reshaped layer params\n",
    "#################################\n",
    "\n",
    "# feature_extract = True\n",
    "feature_extract = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 495,
     "status": "ok",
     "timestamp": 1650105728010,
     "user": {
      "displayName": "whatififif whatififif",
      "userId": "16442723417007489448"
     },
     "user_tz": -600
    },
    "id": "EIyC_0ffAO4a"
   },
   "outputs": [],
   "source": [
    "#################################\n",
    "# function for model training\n",
    "#################################\n",
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    best_acc_test = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'valid' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'valid':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    time_result = 'Time: {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60)\n",
    "    acc_result = 'Acc: {:4f}'.format(best_acc)\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history, time_result, acc_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1650105728011,
     "user": {
      "displayName": "whatififif whatififif",
      "userId": "16442723417007489448"
     },
     "user_tz": -600
    },
    "id": "op8Y04oRA0rp"
   },
   "outputs": [],
   "source": [
    "#################################\n",
    "# function to set the requires_grad parameter\n",
    "#################################\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "f13d9c98f7b34354a78aed76abad7537",
      "579a2606804644e9a16b7cf5c02ed44e",
      "c6032f9ec75e49d181031135bc836b63",
      "0e5d42783e864ea589499117454626a9",
      "174682da4ba94beab319e548eec250fa",
      "d57eaf16f02540478a110e74576b100b",
      "4b3de1ea544a484ba3fbac1d9dd44518",
      "7d9dc63831864e7a8ddeff3e710b77aa",
      "88fd35b0c0a34af089d9aa9dbdbd4894",
      "ac5ae4923a0b446bbc2822f8fce89e7c",
      "37414146bbde4c098eba3a2c3e999bdc"
     ]
    },
    "executionInfo": {
     "elapsed": 5808,
     "status": "ok",
     "timestamp": 1650105733814,
     "user": {
      "displayName": "whatififif whatififif",
      "userId": "16442723417007489448"
     },
     "user_tz": -600
    },
    "id": "yEKmFZiVA23D",
    "outputId": "a10eae6f-9eed-4934-c46d-80412ca838a8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/lukemelas/PyTorch-Pretrained-ViT/releases/download/0.0.2/B_16_imagenet1k.pth\" to /root/.cache/torch/hub/checkpoints/B_16_imagenet1k.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f13d9c98f7b34354a78aed76abad7537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/331M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights.\n",
      "ViT(\n",
      "  (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "  (positional_embedding): PositionalEmbedding1D()\n",
      "  (transformer): Transformer(\n",
      "    (blocks): ModuleList(\n",
      "      (0): Block(\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (pwff): PositionWiseFeedForward(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): Block(\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (pwff): PositionWiseFeedForward(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): Block(\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (pwff): PositionWiseFeedForward(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (3): Block(\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (pwff): PositionWiseFeedForward(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (4): Block(\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (pwff): PositionWiseFeedForward(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (5): Block(\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (pwff): PositionWiseFeedForward(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (6): Block(\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (pwff): PositionWiseFeedForward(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (7): Block(\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (pwff): PositionWiseFeedForward(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (8): Block(\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (pwff): PositionWiseFeedForward(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (9): Block(\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (pwff): PositionWiseFeedForward(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (10): Block(\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (pwff): PositionWiseFeedForward(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (11): Block(\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (pwff): PositionWiseFeedForward(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (fc): Linear(in_features=768, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#################################\n",
    "# function to get the model from pytorch models\n",
    "#################################\n",
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet18\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet\n",
    "        \"\"\"\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"squeezenet\":\n",
    "        \"\"\" Squeezenet\n",
    "        \"\"\"\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\"\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes) \n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"inception\":\n",
    "        \"\"\" Inception v3 \n",
    "        Be careful, expects (299,299) sized images and has auxiliary output\n",
    "        \"\"\"\n",
    "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # Handle the auxilary net\n",
    "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # Handle the primary net\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 299\n",
    "    elif model_name == \"vit\":\n",
    "        \"\"\" Inception v3 \n",
    "        Be careful, expects (299,299) sized images and has auxiliary output\n",
    "        \"\"\"\n",
    "        # config = dict(hidden_size=512, num_heads=8, num_layers=6)\n",
    "        # model = ViT.from_config(config)\n",
    "        model_ft = ViT('B_16_imagenet1k', pretrained=use_pretrained)\n",
    "\n",
    "        # model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # # Handle the auxilary net\n",
    "        # num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "        # model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # # Handle the primary net\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 384\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "    \n",
    "    return model_ft, input_size\n",
    "\n",
    "#################################\n",
    "# Initialize the model for this run\n",
    "#################################\n",
    "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "\n",
    "# Print the model we just instantiated\n",
    "print(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4579,
     "status": "ok",
     "timestamp": 1650105738384,
     "user": {
      "displayName": "whatififif whatififif",
      "userId": "16442723417007489448"
     },
     "user_tz": -600
    },
    "id": "HB6CfnPZA74Z",
    "outputId": "f81d4961-3f29-46b5-9915-68b162169127"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Datasets and Dataloaders...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    }
   ],
   "source": [
    "#################################\n",
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "#################################\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "# Create training and validation datasets\n",
    "# image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'valid']}\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'valid', 'test']}\n",
    "# Create training and validation dataloaders\n",
    "# dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'valid']}\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'valid', 'test']}\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1650105738386,
     "user": {
      "displayName": "whatififif whatififif",
      "userId": "16442723417007489448"
     },
     "user_tz": -600
    },
    "id": "UYgUszLSCM0D",
    "outputId": "8f8f03ab-1753-4eac-8d45-3de92a22d6d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t class_token\n",
      "\t patch_embedding.weight\n",
      "\t patch_embedding.bias\n",
      "\t positional_embedding.pos_embedding\n",
      "\t transformer.blocks.0.attn.proj_q.weight\n",
      "\t transformer.blocks.0.attn.proj_q.bias\n",
      "\t transformer.blocks.0.attn.proj_k.weight\n",
      "\t transformer.blocks.0.attn.proj_k.bias\n",
      "\t transformer.blocks.0.attn.proj_v.weight\n",
      "\t transformer.blocks.0.attn.proj_v.bias\n",
      "\t transformer.blocks.0.proj.weight\n",
      "\t transformer.blocks.0.proj.bias\n",
      "\t transformer.blocks.0.norm1.weight\n",
      "\t transformer.blocks.0.norm1.bias\n",
      "\t transformer.blocks.0.pwff.fc1.weight\n",
      "\t transformer.blocks.0.pwff.fc1.bias\n",
      "\t transformer.blocks.0.pwff.fc2.weight\n",
      "\t transformer.blocks.0.pwff.fc2.bias\n",
      "\t transformer.blocks.0.norm2.weight\n",
      "\t transformer.blocks.0.norm2.bias\n",
      "\t transformer.blocks.1.attn.proj_q.weight\n",
      "\t transformer.blocks.1.attn.proj_q.bias\n",
      "\t transformer.blocks.1.attn.proj_k.weight\n",
      "\t transformer.blocks.1.attn.proj_k.bias\n",
      "\t transformer.blocks.1.attn.proj_v.weight\n",
      "\t transformer.blocks.1.attn.proj_v.bias\n",
      "\t transformer.blocks.1.proj.weight\n",
      "\t transformer.blocks.1.proj.bias\n",
      "\t transformer.blocks.1.norm1.weight\n",
      "\t transformer.blocks.1.norm1.bias\n",
      "\t transformer.blocks.1.pwff.fc1.weight\n",
      "\t transformer.blocks.1.pwff.fc1.bias\n",
      "\t transformer.blocks.1.pwff.fc2.weight\n",
      "\t transformer.blocks.1.pwff.fc2.bias\n",
      "\t transformer.blocks.1.norm2.weight\n",
      "\t transformer.blocks.1.norm2.bias\n",
      "\t transformer.blocks.2.attn.proj_q.weight\n",
      "\t transformer.blocks.2.attn.proj_q.bias\n",
      "\t transformer.blocks.2.attn.proj_k.weight\n",
      "\t transformer.blocks.2.attn.proj_k.bias\n",
      "\t transformer.blocks.2.attn.proj_v.weight\n",
      "\t transformer.blocks.2.attn.proj_v.bias\n",
      "\t transformer.blocks.2.proj.weight\n",
      "\t transformer.blocks.2.proj.bias\n",
      "\t transformer.blocks.2.norm1.weight\n",
      "\t transformer.blocks.2.norm1.bias\n",
      "\t transformer.blocks.2.pwff.fc1.weight\n",
      "\t transformer.blocks.2.pwff.fc1.bias\n",
      "\t transformer.blocks.2.pwff.fc2.weight\n",
      "\t transformer.blocks.2.pwff.fc2.bias\n",
      "\t transformer.blocks.2.norm2.weight\n",
      "\t transformer.blocks.2.norm2.bias\n",
      "\t transformer.blocks.3.attn.proj_q.weight\n",
      "\t transformer.blocks.3.attn.proj_q.bias\n",
      "\t transformer.blocks.3.attn.proj_k.weight\n",
      "\t transformer.blocks.3.attn.proj_k.bias\n",
      "\t transformer.blocks.3.attn.proj_v.weight\n",
      "\t transformer.blocks.3.attn.proj_v.bias\n",
      "\t transformer.blocks.3.proj.weight\n",
      "\t transformer.blocks.3.proj.bias\n",
      "\t transformer.blocks.3.norm1.weight\n",
      "\t transformer.blocks.3.norm1.bias\n",
      "\t transformer.blocks.3.pwff.fc1.weight\n",
      "\t transformer.blocks.3.pwff.fc1.bias\n",
      "\t transformer.blocks.3.pwff.fc2.weight\n",
      "\t transformer.blocks.3.pwff.fc2.bias\n",
      "\t transformer.blocks.3.norm2.weight\n",
      "\t transformer.blocks.3.norm2.bias\n",
      "\t transformer.blocks.4.attn.proj_q.weight\n",
      "\t transformer.blocks.4.attn.proj_q.bias\n",
      "\t transformer.blocks.4.attn.proj_k.weight\n",
      "\t transformer.blocks.4.attn.proj_k.bias\n",
      "\t transformer.blocks.4.attn.proj_v.weight\n",
      "\t transformer.blocks.4.attn.proj_v.bias\n",
      "\t transformer.blocks.4.proj.weight\n",
      "\t transformer.blocks.4.proj.bias\n",
      "\t transformer.blocks.4.norm1.weight\n",
      "\t transformer.blocks.4.norm1.bias\n",
      "\t transformer.blocks.4.pwff.fc1.weight\n",
      "\t transformer.blocks.4.pwff.fc1.bias\n",
      "\t transformer.blocks.4.pwff.fc2.weight\n",
      "\t transformer.blocks.4.pwff.fc2.bias\n",
      "\t transformer.blocks.4.norm2.weight\n",
      "\t transformer.blocks.4.norm2.bias\n",
      "\t transformer.blocks.5.attn.proj_q.weight\n",
      "\t transformer.blocks.5.attn.proj_q.bias\n",
      "\t transformer.blocks.5.attn.proj_k.weight\n",
      "\t transformer.blocks.5.attn.proj_k.bias\n",
      "\t transformer.blocks.5.attn.proj_v.weight\n",
      "\t transformer.blocks.5.attn.proj_v.bias\n",
      "\t transformer.blocks.5.proj.weight\n",
      "\t transformer.blocks.5.proj.bias\n",
      "\t transformer.blocks.5.norm1.weight\n",
      "\t transformer.blocks.5.norm1.bias\n",
      "\t transformer.blocks.5.pwff.fc1.weight\n",
      "\t transformer.blocks.5.pwff.fc1.bias\n",
      "\t transformer.blocks.5.pwff.fc2.weight\n",
      "\t transformer.blocks.5.pwff.fc2.bias\n",
      "\t transformer.blocks.5.norm2.weight\n",
      "\t transformer.blocks.5.norm2.bias\n",
      "\t transformer.blocks.6.attn.proj_q.weight\n",
      "\t transformer.blocks.6.attn.proj_q.bias\n",
      "\t transformer.blocks.6.attn.proj_k.weight\n",
      "\t transformer.blocks.6.attn.proj_k.bias\n",
      "\t transformer.blocks.6.attn.proj_v.weight\n",
      "\t transformer.blocks.6.attn.proj_v.bias\n",
      "\t transformer.blocks.6.proj.weight\n",
      "\t transformer.blocks.6.proj.bias\n",
      "\t transformer.blocks.6.norm1.weight\n",
      "\t transformer.blocks.6.norm1.bias\n",
      "\t transformer.blocks.6.pwff.fc1.weight\n",
      "\t transformer.blocks.6.pwff.fc1.bias\n",
      "\t transformer.blocks.6.pwff.fc2.weight\n",
      "\t transformer.blocks.6.pwff.fc2.bias\n",
      "\t transformer.blocks.6.norm2.weight\n",
      "\t transformer.blocks.6.norm2.bias\n",
      "\t transformer.blocks.7.attn.proj_q.weight\n",
      "\t transformer.blocks.7.attn.proj_q.bias\n",
      "\t transformer.blocks.7.attn.proj_k.weight\n",
      "\t transformer.blocks.7.attn.proj_k.bias\n",
      "\t transformer.blocks.7.attn.proj_v.weight\n",
      "\t transformer.blocks.7.attn.proj_v.bias\n",
      "\t transformer.blocks.7.proj.weight\n",
      "\t transformer.blocks.7.proj.bias\n",
      "\t transformer.blocks.7.norm1.weight\n",
      "\t transformer.blocks.7.norm1.bias\n",
      "\t transformer.blocks.7.pwff.fc1.weight\n",
      "\t transformer.blocks.7.pwff.fc1.bias\n",
      "\t transformer.blocks.7.pwff.fc2.weight\n",
      "\t transformer.blocks.7.pwff.fc2.bias\n",
      "\t transformer.blocks.7.norm2.weight\n",
      "\t transformer.blocks.7.norm2.bias\n",
      "\t transformer.blocks.8.attn.proj_q.weight\n",
      "\t transformer.blocks.8.attn.proj_q.bias\n",
      "\t transformer.blocks.8.attn.proj_k.weight\n",
      "\t transformer.blocks.8.attn.proj_k.bias\n",
      "\t transformer.blocks.8.attn.proj_v.weight\n",
      "\t transformer.blocks.8.attn.proj_v.bias\n",
      "\t transformer.blocks.8.proj.weight\n",
      "\t transformer.blocks.8.proj.bias\n",
      "\t transformer.blocks.8.norm1.weight\n",
      "\t transformer.blocks.8.norm1.bias\n",
      "\t transformer.blocks.8.pwff.fc1.weight\n",
      "\t transformer.blocks.8.pwff.fc1.bias\n",
      "\t transformer.blocks.8.pwff.fc2.weight\n",
      "\t transformer.blocks.8.pwff.fc2.bias\n",
      "\t transformer.blocks.8.norm2.weight\n",
      "\t transformer.blocks.8.norm2.bias\n",
      "\t transformer.blocks.9.attn.proj_q.weight\n",
      "\t transformer.blocks.9.attn.proj_q.bias\n",
      "\t transformer.blocks.9.attn.proj_k.weight\n",
      "\t transformer.blocks.9.attn.proj_k.bias\n",
      "\t transformer.blocks.9.attn.proj_v.weight\n",
      "\t transformer.blocks.9.attn.proj_v.bias\n",
      "\t transformer.blocks.9.proj.weight\n",
      "\t transformer.blocks.9.proj.bias\n",
      "\t transformer.blocks.9.norm1.weight\n",
      "\t transformer.blocks.9.norm1.bias\n",
      "\t transformer.blocks.9.pwff.fc1.weight\n",
      "\t transformer.blocks.9.pwff.fc1.bias\n",
      "\t transformer.blocks.9.pwff.fc2.weight\n",
      "\t transformer.blocks.9.pwff.fc2.bias\n",
      "\t transformer.blocks.9.norm2.weight\n",
      "\t transformer.blocks.9.norm2.bias\n",
      "\t transformer.blocks.10.attn.proj_q.weight\n",
      "\t transformer.blocks.10.attn.proj_q.bias\n",
      "\t transformer.blocks.10.attn.proj_k.weight\n",
      "\t transformer.blocks.10.attn.proj_k.bias\n",
      "\t transformer.blocks.10.attn.proj_v.weight\n",
      "\t transformer.blocks.10.attn.proj_v.bias\n",
      "\t transformer.blocks.10.proj.weight\n",
      "\t transformer.blocks.10.proj.bias\n",
      "\t transformer.blocks.10.norm1.weight\n",
      "\t transformer.blocks.10.norm1.bias\n",
      "\t transformer.blocks.10.pwff.fc1.weight\n",
      "\t transformer.blocks.10.pwff.fc1.bias\n",
      "\t transformer.blocks.10.pwff.fc2.weight\n",
      "\t transformer.blocks.10.pwff.fc2.bias\n",
      "\t transformer.blocks.10.norm2.weight\n",
      "\t transformer.blocks.10.norm2.bias\n",
      "\t transformer.blocks.11.attn.proj_q.weight\n",
      "\t transformer.blocks.11.attn.proj_q.bias\n",
      "\t transformer.blocks.11.attn.proj_k.weight\n",
      "\t transformer.blocks.11.attn.proj_k.bias\n",
      "\t transformer.blocks.11.attn.proj_v.weight\n",
      "\t transformer.blocks.11.attn.proj_v.bias\n",
      "\t transformer.blocks.11.proj.weight\n",
      "\t transformer.blocks.11.proj.bias\n",
      "\t transformer.blocks.11.norm1.weight\n",
      "\t transformer.blocks.11.norm1.bias\n",
      "\t transformer.blocks.11.pwff.fc1.weight\n",
      "\t transformer.blocks.11.pwff.fc1.bias\n",
      "\t transformer.blocks.11.pwff.fc2.weight\n",
      "\t transformer.blocks.11.pwff.fc2.bias\n",
      "\t transformer.blocks.11.norm2.weight\n",
      "\t transformer.blocks.11.norm2.bias\n",
      "\t norm.weight\n",
      "\t norm.bias\n",
      "\t fc.weight\n",
      "\t fc.bias\n"
     ]
    }
   ],
   "source": [
    "#################################\n",
    "# Send the model to GPU\n",
    "#################################\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "\n",
    "#################################\n",
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are \n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "#################################\n",
    "\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "#################################\n",
    "# Observe that all parameters are being optimized\n",
    "#################################\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7FmqqROPCNmi",
    "outputId": "bb12ffc1-08d7-4dfd-e66d-a9163bdb2972"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    }
   ],
   "source": [
    "#################################\n",
    "# Setup the loss fxn\n",
    "#################################\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "#################################\n",
    "# Train and evaluate\n",
    "#################################\n",
    "\n",
    "# model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))\n",
    "model_ft, hist, time_result, acc_result = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kpTlM6nsDmp6"
   },
   "outputs": [],
   "source": [
    "#################################\n",
    "# function to test model on test sample\n",
    "#################################\n",
    "\n",
    "def test_model(model, dataloaders):\n",
    "  # Iterate over data.\n",
    "  phase = 'test'\n",
    "  running_corrects = 0\n",
    "  for inputs, labels in dataloaders[phase]:\n",
    "      inputs = inputs.to(device)\n",
    "      labels = labels.to(device)\n",
    "      outputs = model(inputs)\n",
    "      _, preds = torch.max(outputs, 1)\n",
    "      running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "  test_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "  test_acc_result = 'Test: {:4f}'.format(test_acc)\n",
    "\n",
    "  return test_acc_result, preds\n",
    "\n",
    "test_acc_result, preds = test_model(model_ft, dataloaders_dict)\n",
    "print(test_acc_result)\n",
    "print(\"preds: \", preds )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rM6MAq5i0qJM"
   },
   "outputs": [],
   "source": [
    "#################################\n",
    "# function to test model on real test sample ( X_test.npy)\n",
    "#################################\n",
    "def real_test_model(model, real_test_samples_folder, images_folder, input_size, save_filename):\n",
    "    \n",
    "#     with open('imagenet_classes.txt') as f:\n",
    "#       labels = [line.strip() for line in f.readlines()]\n",
    "    labels = [0, 1, 2, 3]\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    data_transforms = {\n",
    "        'real_test': transforms.Compose([\n",
    "            transforms.Resize(input_size),\n",
    "            transforms.CenterCrop(input_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    } \n",
    "\n",
    "    test_img_list = []\n",
    "    predictions = []\n",
    "    for filename in glob.glob(f'{real_test_samples_folder}/{images_folder}/*.jpg'): # assuming jpg\n",
    "        img=Image.open(filename)\n",
    "        img_t = data_transforms['real_test'](img)\n",
    "        batch_t = torch.unsqueeze(img_t, 0)\n",
    "        batch_t = batch_t.to(device)\n",
    "        out = model(batch_t)\n",
    "        _, index = torch.max(out, 1)\n",
    "        percentage = torch.nn.functional.softmax(out, dim=1)[0] * 100\n",
    "        print(labels[index[0]], percentage[index[0]].item())\n",
    "        test_img_list.append(filename.split('/')[-1])\n",
    "        predictions.append(labels[index[0]])\n",
    "\n",
    "    print('image list:', test_img_list)\n",
    "    print('predictions:', predictions)\n",
    "    np.save(f'{real_test_samples_folder}/{save_filename}', np.array(predictions))\n",
    "\n",
    "real_test_samples_folder = '/content/drive/MyDrive/shared/samples_x384_all_real_test'\n",
    "images_folder = 'real_test'\n",
    "\n",
    "# real_test_samples_folder = '/content/drive/MyDrive/shared/samples_x384_all/valid'\n",
    "# images_folder = '3'\n",
    "\n",
    "# input_size = 224 # for other models\n",
    "input_size = 384 # for ViT\n",
    "save_filename = 'X_test_preds1.npy'\n",
    "\n",
    "real_test_model(model_ft, real_test_samples_folder, images_folder, input_size, save_filename)\n",
    "read_again = np.load(f'{real_test_samples_folder}/{save_filename}')\n",
    "print('read_again', read_again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MgyPReorC3Ue"
   },
   "outputs": [],
   "source": [
    "#################################\n",
    "# For comparison purpose between non-trained version vs pretrained version.\n",
    "# Initialize the non-pretrained version of the model used for this run\n",
    "#################################\n",
    "\n",
    "scratch_model,_ = initialize_model(model_name, num_classes, feature_extract=False, use_pretrained=False)\n",
    "scratch_model = scratch_model.to(device)\n",
    "scratch_optimizer = optim.SGD(scratch_model.parameters(), lr=0.001, momentum=0.9)\n",
    "scratch_criterion = nn.CrossEntropyLoss()\n",
    "# _,scratch_hist = train_model(scratch_model, dataloaders_dict, scratch_criterion, scratch_optimizer, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))\n",
    "_,scratch_hist, _, _ = train_model(scratch_model, dataloaders_dict, scratch_criterion, scratch_optimizer, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))\n",
    "\n",
    "# Plot the training curves of validation accuracy vs. number \n",
    "#  of training epochs for the transfer learning method and\n",
    "#  the model trained from scratch\n",
    "ohist = []\n",
    "shist = []\n",
    "\n",
    "ohist = [h.cpu().numpy() for h in hist]\n",
    "shist = [h.cpu().numpy() for h in scratch_hist]\n",
    "\n",
    "# plt.title(f\"Validation Accuracy vs. Number of Training Epochs: {model_name}\")\n",
    "plt.title(f\"Model: {model_name}, {time_result}, {acc_result}, {test_acc_result}\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.plot(range(1,num_epochs+1),ohist,label=\"Pretrained\")\n",
    "plt.plot(range(1,num_epochs+1),shist,label=\"Scratch\")\n",
    "plt.ylim((0,1.))\n",
    "plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TaoLt4BoGsAe"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "phs_pytorch_models_vit_real_test.ipynb",
   "provenance": [
    {
     "file_id": "1CAZ8BFYA4kuWqK2JZJQ_LUpv0cJf2iV6",
     "timestamp": 1649772271388
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0e5d42783e864ea589499117454626a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ac5ae4923a0b446bbc2822f8fce89e7c",
      "placeholder": "​",
      "style": "IPY_MODEL_37414146bbde4c098eba3a2c3e999bdc",
      "value": " 331M/331M [00:02&lt;00:00, 124MB/s]"
     }
    },
    "174682da4ba94beab319e548eec250fa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "37414146bbde4c098eba3a2c3e999bdc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4b3de1ea544a484ba3fbac1d9dd44518": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "579a2606804644e9a16b7cf5c02ed44e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d57eaf16f02540478a110e74576b100b",
      "placeholder": "​",
      "style": "IPY_MODEL_4b3de1ea544a484ba3fbac1d9dd44518",
      "value": "100%"
     }
    },
    "7d9dc63831864e7a8ddeff3e710b77aa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "88fd35b0c0a34af089d9aa9dbdbd4894": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ac5ae4923a0b446bbc2822f8fce89e7c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c6032f9ec75e49d181031135bc836b63": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7d9dc63831864e7a8ddeff3e710b77aa",
      "max": 347469964,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_88fd35b0c0a34af089d9aa9dbdbd4894",
      "value": 347469964
     }
    },
    "d57eaf16f02540478a110e74576b100b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f13d9c98f7b34354a78aed76abad7537": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_579a2606804644e9a16b7cf5c02ed44e",
       "IPY_MODEL_c6032f9ec75e49d181031135bc836b63",
       "IPY_MODEL_0e5d42783e864ea589499117454626a9"
      ],
      "layout": "IPY_MODEL_174682da4ba94beab319e548eec250fa"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
